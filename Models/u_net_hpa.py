# -*- coding: utf-8 -*-
"""U_Net_HPA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_NtgrXFF7ejo5jTLBcRBWPy7B19kE1m
"""

HPA_file_path = ''  # path to map containing 'images_train.npz' etc
working_dir = ''  # path to working directory

# Commented out IPython magic to ensure Python compatibility.
# Torch and torchvision imports
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision
from torchvision import datasets, transforms
from torchvision.transforms import ToTensor

# Scikit-image, scipy, Scikit-learn imports
from skimage import io, filters, measure, morphology, segmentation, color, feature
from skimage.filters import threshold_li, gaussian, threshold_otsu
from skimage.feature import peak_local_max, canny
from skimage.morphology import remove_small_objects
from skimage.transform import resize, AffineTransform, warp
from skimage.metrics import hausdorff_distance
from scipy.ndimage import distance_transform_edt, rotate
from scipy import ndimage as ndi
from scipy.spatial.distance import directed_hausdorff
from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, f1_score

# miscellaneous imports
import numpy as np
import matplotlib
matplotlib.rcParams["image.interpolation"] = 'none'
import matplotlib.pyplot as plt
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
from PIL import Image
import cv2
import random
from tqdm.auto import tqdm
import os
import albumentations as A
import re

# Device agnostic
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Unet model
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv_op = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.conv_op(x)


class DownSample(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = DoubleConv(in_channels, out_channels)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        down = self.conv(x)
        p = self.pool(down)

        return down, p


class UpSample(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.up = nn.ConvTranspose2d(
            in_channels, in_channels // 2, kernel_size=2, stride=2
        )
        self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        x = torch.cat([x1, x2], 1)
        return self.conv(x)

# classic UNet
class UNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.down_convolution_1 = DownSample(1, 64)
        self.down_convolution_2 = DownSample(64, 128)
        self.down_convolution_3 = DownSample(128, 256)
        self.down_convolution_4 = DownSample(256, 512)

        self.bottle_neck = DoubleConv(512, 1024)

        self.up_convolution_1 = UpSample(1024, 512)
        self.up_convolution_2 = UpSample(512, 256)
        self.up_convolution_3 = UpSample(256, 128)
        self.up_convolution_4 = UpSample(128, 64)

        self.out = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)

    def forward(self, x):
        down_1, p1 = self.down_convolution_1(x)
        down_2, p2 = self.down_convolution_2(p1)
        down_3, p3 = self.down_convolution_3(p2)
        down_4, p4 = self.down_convolution_4(p3)

        b = self.bottle_neck(p4)

        up_1 = self.up_convolution_1(b, down_4)
        up_2 = self.up_convolution_2(up_1, down_3)
        up_3 = self.up_convolution_3(up_2, down_2)
        up_4 = self.up_convolution_4(up_3, down_1)

        out = self.out(up_4)
        out = torch.sigmoid(out)

        return out

# UNet based on VGG16 encoder
class VGG16_UNet(nn.Module):
    def __init__(self):
        super().__init__()
        vgg16 = torchvision.models.vgg16(weights='DEFAULT')
        self.pool = nn.MaxPool2d(2, 2)

        # Modify the first convolutional layer to accept 1 channel instead of 3
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            *vgg16.features[1:]
        )
        # Initialize weights from pretrained VGG16 model
        self.encoder[0].weight.data = vgg16.features[0].weight.mean(dim=1, keepdim=True)
        self.encoder[0].bias.data = vgg16.features[0].bias.data

        # Define convolutional blocks corresponding to VGG layers
        self.conv_1 = nn.Sequential(*self.encoder[:4])
        self.conv_2 = nn.Sequential(*self.encoder[5:9])
        self.conv_3 = nn.Sequential(*self.encoder[10:16])
        self.conv_4 = nn.Sequential(*self.encoder[17:23])
        self.conv_5 = nn.Sequential(*self.encoder[24:30])

        self.bottle_neck = DoubleConv(512, 1024)

        self.up_convolution_1 = UpSample(512 + 512, 1024)
        self.up_convolution_2 = UpSample(512 + 512, 512)
        self.up_convolution_3 = UpSample(256 + 256, 256)
        self.up_convolution_4 = UpSample(128 + 128, 128)
        self.up_convolution_5 = UpSample(64 + 64, 64)

        self.out = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)

    def forward(self, x):
        down_1 = self.conv_1(x)                      # 1 > 64
        down_2 = self.conv_2(self.pool(down_1))      # 64 > 128
        down_3 = self.conv_3(self.pool(down_2))      # 128 > 256
        down_4 = self.conv_4(self.pool(down_3))      # 256 > 512
        down_5 = self.conv_5(self.pool(down_4))      # 512 > 512

        b = self.bottle_neck(self.pool(down_5))

        up_1 = self.up_convolution_1(b, down_5)
        up_2 = self.up_convolution_2(up_1, down_4)
        up_3 = self.up_convolution_3(up_2, down_3)
        up_4 = self.up_convolution_4(up_3, down_2)
        up_5 = self.up_convolution_5(up_4, down_1)

        out = self.out(up_5)
        out = torch.sigmoid(out)

        return out

# UNet based on ResNet34 encoder
class ResNet34_UNet(nn.Module):
    def __init__(self):
        super().__init__()
        resnet = torchvision.models.resnet34(weights='DEFAULT')
        self.pool = resnet.maxpool

        # Modify the first convolutional layer to accept 1 channel instead of 3
        self.layer0 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),
            resnet.bn1,
            resnet.relu
        )
        # Initialize weights from pretrained ResNet34 model
        self.layer0[0].weight.data = resnet.conv1.weight.mean(dim=1, keepdim=True)

        self.layer1 = resnet.layer1
        self.layer2 = resnet.layer2
        self.layer3 = resnet.layer3
        self.layer4 = resnet.layer4

        self.bottle_neck = DoubleConv(512, 1024)

        self.up_convolution_1 = UpSample(1024, 512)
        self.up_convolution_2 = UpSample(512, 256)
        self.up_convolution_3 = UpSample(256, 128)
        self.up_convolution_4 = UpSample(128, 64)

        self.out = nn.Sequential(
            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)
        )

    def forward(self, x):
        down_0 = self.layer0(x)                      # 1 > 64
        down_1 = self.layer1(down_0)                 # 64 > 64
        down_2 = self.layer2(down_1)                 # 64 > 128
        down_3 = self.layer3(down_2)                 # 128 > 256
        down_4 = self.layer4(down_3)                 # 256 > 512

        b = self.bottle_neck(self.pool(down_4))

        up_1 = self.up_convolution_1(b, down_4)
        up_2 = self.up_convolution_2(up_1, down_3)
        up_3 = self.up_convolution_3(up_2, down_2)
        up_4 = self.up_convolution_4(up_3, down_1)

        out = self.out(up_4)
        out = torch.sigmoid(out)

        return out

class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True, loss_type='combined'):
        super(DiceBCELoss, self).__init__()
        self.loss_type = loss_type  # Initialize with a default loss_type

    def forward(self, inputs, targets, smooth=1, loss_type=None):
        """
        loss_type can be 'combined', 'dice', or 'bce' to specify which loss is returned first.
        If not provided, it will default to the loss_type set during initialization.
        """
        # If loss_type is provided during the forward pass, use it; otherwise, use the initialized loss_type
        if loss_type is None:
            loss_type = self.loss_type

        # Flatten label and prediction tensors
        inputs = inputs.reshape(-1)
        targets = targets.reshape(-1)

        # Dice loss
        intersection = (inputs * targets).sum()
        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)

        # Calculate weights
        weights = torch.where(targets == 1, 2.0, 1.0)

        # BCE loss
        BCE = F.binary_cross_entropy(inputs, targets, weight=weights, reduction='mean')

        # Combine Dice loss and BCE loss
        Dice_BCE = BCE + dice_loss

        # Return the losses
        if loss_type == 'combined':
            return Dice_BCE, dice_loss, BCE
        elif loss_type == 'dice':
            return dice_loss, dice_loss, BCE
        elif loss_type == 'bce':
            return BCE, dice_loss, BCE
        else:
            raise ValueError("loss_type must be 'combined', 'dice', or 'bce'")

def train_model(model, train_loader, loss_fn, optimizer):
    # Initialize cumulative loss values
    train_loss = 0
    train_dice_loss = 0
    train_BCE = 0

    # Set the model to training mode
    model.train()

    for img in train_loader:
        # Extract input image (X) and target mask (y)
        X = img[:, 0, :, :].unsqueeze(1)
        y = img[:, 1, :, :]

        # Forward pass: Predict output mask using the model
        y_pred = model(X).squeeze()

        # Calculate the losses
        loss, dice_loss, BCE = loss_fn(y_pred, y)
        train_loss += loss.item()
        train_dice_loss += dice_loss.item()
        train_BCE += BCE.item()

        # Backward pass and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Calculate average losses over the entire training set
    train_loss /= len(train_loader)
    train_dice_loss /= len(train_loader)
    train_BCE /= len(train_loader)

    return train_loss, train_dice_loss, train_BCE

def test_model(model, test_loader, loss_fn):
    # Initialize cumulative loss values
    test_loss = 0
    test_dice_loss = 0
    test_BCE = 0

    # Set the model to evaluation mode
    model.eval()

    with torch.no_grad():
        for img in test_loader:
            # Extract input image (X_test) and target mask (y_test)
            X_test = img[:, 0, :, :].unsqueeze(1)
            y_test = img[:, 1, :, :]

            # Forward pass: Predict output mask using the model
            test_pred = model(X_test).squeeze()

            # Calculate the losses
            loss, dice_loss, BCE = loss_fn(test_pred, y_test)
            test_loss += loss.item()
            test_dice_loss += dice_loss.item()
            test_BCE += BCE.item()

    # Calculate average losses over the entire test set
    test_loss /= len(test_loader)
    test_dice_loss /= len(test_loader)
    test_BCE /= len(test_loader)

    return test_loss, test_dice_loss, test_BCE

def fit_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=5):
    val_loss_list = []
    train_loss_list = []

    for epoch in tqdm(range(epochs)):
        print(f'Epoch {epoch + 1}/{epochs}')

        # Train the model and get training losses
        train_loss, dice_train, BCE_train = train_model(model, train_loader, loss_fn, optimizer)
        train_loss_list.append(round(train_loss, 4))

        # Evaluate the model on the val set and get val losses
        val_loss, dice_val, BCE_val = test_model(model, val_loader, loss_fn)
        val_loss_list.append(round(val_loss, 4))

        # Print the training and val losses for this epoch
        print(f'Training Loss: {train_loss:.4f} (DICE: {dice_train:.4f}, BCE: {BCE_train:.4f}) | val Loss: {val_loss:.4f} (DICE: {dice_val:.4f}, BCE: {BCE_val:.4f})')

    return train_loss_list, val_loss_list

def plot_loss(train_loss_list, val_loss_list):
    # Plot the training and val loss curves over epochs
    epochs = range(1, len(train_loss_list) + 1)

    plt.plot(epochs, train_loss_list, 'b', label='Training Loss')
    plt.plot(epochs, val_loss_list, 'r', label='val Loss')
    plt.title('Training and val Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

def evaluate_model(model, data_loader):
    model.eval()  # Set the model to evaluation mode

    # Initialize lists to store per-sample metrics
    sensitivity_list, specificity_list, dice_list, hd_list = [], [], [], []

    with torch.no_grad():
        for img in data_loader:
            # Extract input image (X_test) and target mask (y_test)
            X_test = img[:, 0, :, :].unsqueeze(1)
            y_test = img[:, 1, :, :].to(torch.int)

            # Forward pass: Predict output mask using the model
            test_pred = model(X_test).squeeze()

            # Binarize the predictions based on a threshold (0.5)
            test_pred = (test_pred >= 0.5).to(torch.int)

            # Move to numpy for processing
            test_pred_np = test_pred.cpu().numpy()
            y_test_np = y_test.cpu().numpy()

            for i in range(X_test.shape[0]):
                # Get the boundary pixels using the Canny edge detector
                edges_y = canny(y_test_np[i].astype(bool))
                edges_pred = canny(test_pred_np[i].astype(bool))

                # Compute the Hausdorff distance in both directions
                d1 = hausdorff_distance(edges_y, edges_pred)
                d2 = hausdorff_distance(edges_pred, edges_y)
                hd_list.append(max(d1, d2))

                # Flatten the predictions and targets for evaluation
                test_pred_flat = test_pred_np[i].flatten()
                y_test_flat = y_test_np[i].flatten()

                # Compute confusion matrix components
                tn, fp, fn, tp = confusion_matrix(y_test_flat, test_pred_flat, labels=[0, 1]).ravel()

                # Calculate and store metrics for this batch
                sensitivity_list.append(recall_score(y_test_flat, test_pred_flat))
                specificity_list.append(tn / (tn + fp) if (tn + fp) != 0 else 0)
                dice_list.append(f1_score(y_test_flat, test_pred_flat))

    # ignore any inf in hd
    hd_array = np.array(hd_list)[np.isfinite(hd_list)]

    # Calculate mean and standard deviation for each metric
    avg_sensitivity = np.mean(sensitivity_list)
    std_sensitivity = np.std(sensitivity_list)

    avg_specificity = np.mean(specificity_list)
    std_specificity = np.std(specificity_list)

    avg_dice_coefficient = np.mean(dice_list)
    std_dice_coefficient = np.std(dice_list)

    avg_hausdorff_distance = np.mean(hd_array)
    std_hausdorff_distance = np.std(hd_array)

    return [[avg_sensitivity, std_sensitivity],
            [avg_specificity, std_specificity],
            [avg_dice_coefficient, std_dice_coefficient],
            [avg_hausdorff_distance, std_hausdorff_distance]]

def evaluate_sequence(model, images):
    model.eval()  # Set the model to evaluation mode

    # Initialize lists to store per-sample metrics
    sensitivity_list, specificity_list, dice_list, hd_list = [], [], [], []
    predictions = []

    with torch.no_grad():
        for i, img in enumerate(images):
            # Extract input image (X_test) and target mask (y_test)
            X_test = img[0, :, :]
            y_test = img[1, :, :].to(torch.int)

            # Forward pass: Predict output mask using the model
            test_pred = model(X_test.unsqueeze(0).unsqueeze(0)).squeeze()

            # Binarize the predictions based on a threshold (0.5)
            test_pred = (test_pred >= 0.5).to(torch.int)

            # Move to numpy for processing
            test_pred_np = test_pred.cpu().numpy()
            y_test_np = y_test.cpu().numpy()
            predictions.append(test_pred_np)

            # Get the boundary pixels using the Canny edge detector
            edges_y = canny(y_test_np.astype(bool))
            edges_pred = canny(test_pred_np.astype(bool))

            # Compute the Hausdorff distance in both directions
            d1 = hausdorff_distance(edges_y, edges_pred)
            d2 = hausdorff_distance(edges_pred, edges_y)
            hd_list.append(max(d1, d2))

            # Flatten the predictions and targets for evaluation
            test_pred_flat = test_pred_np.flatten()
            y_test_flat = y_test_np.flatten()

            # Compute confusion matrix components
            tn, fp, fn, tp = confusion_matrix(y_test_flat, test_pred_flat, labels=[0, 1]).ravel()

            # Calculate and store metrics for this batch
            sensitivity_list.append(recall_score(y_test_flat, test_pred_flat))
            specificity_list.append(tn / (tn + fp) if (tn + fp) != 0 else 0)
            dice_list.append(f1_score(y_test_flat, test_pred_flat))

    # ignore any inf in hd
    hd_array = np.array(hd_list)[np.isfinite(hd_list)]

    # Calculate mean and standard deviation for each metric
    avg_sensitivity = np.mean(sensitivity_list)
    std_sensitivity = np.std(sensitivity_list)

    avg_specificity = np.mean(specificity_list)
    std_specificity = np.std(specificity_list)

    avg_dice_coefficient = np.mean(dice_list)
    std_dice_coefficient = np.std(dice_list)

    avg_hausdorff_distance = np.mean(hd_array)
    std_hausdorff_distance = np.std(hd_array)

    return [[avg_sensitivity, std_sensitivity],
            [avg_specificity, std_specificity],
            [avg_dice_coefficient, std_dice_coefficient],
            [avg_hausdorff_distance, std_hausdorff_distance]], predictions

# Define the transformations for data augmentation
train_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=random.choice([(0, 0), (90, 90), (180, 180), (270, 270)])),
])

class HPA(Dataset):
    def __init__(self, data, transform=None):

        # Convert numpy arrays to tensors
        self.data = [torch.tensor(d, dtype=torch.float32).to(device) for d in data]
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        # Extract image and mask from the tensor
        sample = self.data[index]

        if self.transform:
            sample = self.transform(sample)

        return sample

# Load the arrays from .npz file
images_train = np.load(os.path.join(HPA_file_path, 'images_train.npz'))
images_test = np.load(os.path.join(HPA_file_path, 'images_test.npz'))
images_val = np.load(os.path.join(HPA_file_path, 'images_val.npz'))

# Convert the loaded arrays back into a list
images_train = [images_train[f'arr_{i}'] for i in range(len(images_train.files))]
images_test = [images_test[f'arr_{i}'] for i in range(len(images_test.files))]
images_val = [images_val[f'arr_{i}'] for i in range(len(images_val.files))]

# Create datasets
train_dataset = HPA(images_train, transform=train_transforms)
val_dataset = HPA(images_val)
test_dataset = HPA(images_test)

# Create loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

# model initialization
loss_type = '' # loss_type must be 'combined', 'dice', or 'bce'
loss_fn = DiceBCELoss(loss_type=loss_type)
model = UNet().to(device)  # model_name must be 'UNet', 'VGG16_UNet', or 'ResNet34_UNet'
optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001)

# train model
train_loss_list, val_loss_list = fit_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=100)
plot_loss(train_loss_list, val_loss_list)

# test model
metrics = evaluate_model(model, val_loader)
metrics.extend([train_loss_list, val_loss_list])

# Retrieve a batch of images from the validation DataLoader
batches = list(test_loader)
test_sample_batch = batches[np.random.randint(len(batches))]

# Set up the figure
fig = plt.figure(figsize=(12, 12))
rows, cols = 3, 3

# Randomly select indices within the batch
random_indices = np.random.choice(len(test_sample_batch), size=rows * cols, replace=False)

for i in range(0, rows * cols, 3):
    cell, mask= test_sample_batch[i]

    # get model prediction
    pred = model(cell.unsqueeze(0).unsqueeze(0).to(device)).squeeze().cpu().detach().numpy()
    pred = (pred >= 0.5).astype(np.uint8)

    # Display the cell image
    fig.add_subplot(rows, cols, i + 1)
    plt.imshow(cell.squeeze().cpu(), cmap='gray')
    plt.axis('off')

    # Display the segmentation image
    fig.add_subplot(rows, cols, i + 2)
    plt.imshow(mask.squeeze().cpu(), cmap='gray')
    plt.axis('off')

    # Display the model prediction
    fig.add_subplot(rows, cols, i + 3)
    plt.imshow(pred, cmap='gray')
    plt.axis('off')

plt.suptitle('Cell, Ground Truth, Prediction')
plt.show()

print(f'mean dice coefficient: {metrics[2][0]}')
print(f'mean hausdorff distance: {metrics[3][0]}')

results_file = os.path.join(working_dir, f'HPA_results_{model.__class__.__name__}_{loss_type}.txt')
model_file = os.path.join(working_dir, f'HPA_model_{model.__class__.__name__}_{loss_type}.pth')

with open(results_file, 'w') as file:
     file.write(f'sensitivity: {metrics[0]}\n'
                f'specificity: {metrics[1]}\n'
                f'dice coefficient: {metrics[2]}\n'
                f'hausdorff distance: {metrics[3]}\n'
                f'train loss: {metrics[4]}\n'
                f'test loss: {metrics[5]}\n\n')

torch.save(model.state_dict(), model_file)

# load in sequence
img_seq_test = np.load(os.path.join(HPA_file_path, 'img_seq_test.npz'))
img_seq_val = np.load(os.path.join(HPA_file_path, 'img_seq_val.npz'))

# Convert the loaded arrays back into a list
img_seq_test = [img_seq_test[f'arr_{i}'] for i in range(len(img_seq_test.files))]
img_seq_val = [img_seq_val[f'arr_{i}'] for i in range(len(img_seq_val.files))]

# evaluate synthetic sequence
results_val = {}
for i, seq in enumerate(tqdm(img_seq_val)):
    seq = HPA(seq)
    metrics, predictions = evaluate_sequence(model, seq)
    metrics.extend([predictions])
    results_val[i] = metrics

#plot results
cell_id = 0

predictions = results_val[cell_id][4]
cell_images = img_seq_val[cell_id]
num_images = len(predictions)

# Create subplots
fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))

for i in range(num_images):
    cell_image = cell_images[i][0]  # Extract cell image
    ground_truth = cell_images[i][1]  # Extract ground truth

    # Plot cell image
    axes[i, 0].imshow(cell_image, cmap='gray')
    axes[i, 0].set_title(f'Cell Image {i+1}')
    axes[i, 0].axis('off')

    # Plot ground truth
    axes[i, 1].imshow(ground_truth, cmap='gray')
    axes[i, 1].set_title(f'Ground Truth {i+1}')
    axes[i, 1].axis('off')

    # Plot prediction
    axes[i, 2].imshow(predictions[i], cmap='gray')
    axes[i, 2].set_title(f'Prediction {i+1}')
    axes[i, 2].axis('off')

plt.tight_layout()
plt.show()

results_file = os.path.join(working_dir, f'HPAseq_results_{model.__class__.__name__}_{loss_type}.txt')

with open(results_file, 'w') as file:
    for cell in results_val.keys():
        stats = results_val[cell]
        file.write(f'sensitivity: {stats[0]}\n'
                    f'specificity: {stats[1]}\n'
                    f'dice coefficient: {stats[2]}\n'
                    f'hausdorff distance: {stats[3]}\n\n')